CVPR 2025에 게재된 'Transformer without Normalization'을 읽고 다시금 Normalization에 대해 자세히 정리해야 할 필요성을 느꼈다.  
RNN부터 이어져 온 시퀀스 모델의 공식과도 같았던 Layer Normalization을 비롯하여, CNN을 필두로 한 이미지 모델에 많이 쓰이는 Batch Normalization 등을 정리함으로써 관련 내용을 총체적으로 복습하고자 한다.

## Batch Normaliztion

배치 정규화는 다음과 같은 단계로 동작한다.  
1. 각 노드에 대해, 배치로 들어온 샘플들의 평균과 분산을 구한다.
2. 평균과 분산을 통해, 아래와 같이 정규화한다. (Epsilon은 분산이 매우 작아져 정규화된 값이 발산하는 것을 방지하기 위한 아주 작은 상수)
  <img width="185" height="93" alt="image" src="https://github.com/user-attachments/assets/23f7c2f8-0d38-478f-bf37-726a47ff91eb" />
 
3. 이후에 감마라는 Scaling Factor를 곱하고, 베타라는 Shift Factor를 더한다.  
   <img width="323" height="110" alt="image" src="https://github.com/user-attachments/assets/65bfe5df-ab0a-4660-bca9-3101f45b5bde" />

4. 훈련 세트에 대해 역전파를 진행할 때, 가중치 행렬과 마찬가지로 배치 정규화의 파라미터인 감마와 베타도 학습에 포함된다.  

배치 정규화가 실제로 어떻게 이루어지는지 아래의 간단한 신경망을 통해서 알아보자. (BATCH_SIZE = 16)

<img width="200" height="300" alt="image" src="https://github.com/user-attachments/assets/0e1c1f1b-ccb2-4dfc-bae4-e34caec1aafc" />

### 학습 과정에서의 동작

사진을 통해서, 미니배치에 들어있는 각 샘플은 각각 3개라는 점을 파악할 수 있다.
만약 배치 정규화가 없다면, 일반적으로 input과 hidden layer 사이의 순전파 과정은 다음과 같다.
> Input -> W와 Bias 계산 -> 활성화 함수 -> Hidden Layer

일반적으로, 배치 정규화는 활성화 함수 앞에 위치해 활성화 함수에 들어갈 input의 범위를 조정한다.  
> Input -> W 계산 -> BatchNorm -> 활성화 함수 -> Hidden Layer

배치 정규화 단계가 활성화 함수 앞에 놓이게 되면, W와 bias로 구성된 선형식에서 차이가 발생하게 된다.  
이후에 다시 설명하겠지만 배치 정규화의 학습 파라미터 중 하나인 **베타가 bias의 역할도 수행하기 때문에** 선형식에서 bias를 제외한 W만 계산한다.

이제 Hidden Layer의 첫 번째 노드에 대해서만 계산 단계를 밟아가면서 감을 잡아보자.
1. 16개의 데이터 샘플 각각에 대해 Hidden Node 1에서 계산된 값 10개를 모은다.
2. 이 10개의 값에 대한 평균($mu_1$)과 분산($mu_2$)를 계산한다.
3. 정규화 공식을 통해 정규화된 값을 계산.
4. 감마를 곱하고 베타를 더한다. (두 파라미터의 초기값은 각각 1과 0이므로, 역전파 전에는 정규화된 값과 동일)

### 감마와 베타

Normalization이라는 용어를 이미 알고 있다면, 감마와 베타를 통해 이미 정규화된 값을 다시금 조정하는 이유가 궁금할 수 있다.  
간단히 설명하자면, **감마와 베타가 없이 단순 정규화한 값은 비선형성(=표현력)을 충분히 살리지 못해** 신경망의 존재 이유를 퇴색시키기 때문이다.  

만약 신경망의 활성화 함수로 시그모이드를 사용할 때, 활성화 함수의 인풋이 사진처럼 몰려있는 상황을 가정해보자.  

<img width="989" height="590" alt="image" src="https://github.com/user-attachments/assets/e2406429-459e-404c-93b9-c15d93b78d01" />

이 빨간 점들의 그래디언트는 0에 가깝다.  
이로 인해 역전파 과정에서 기울기가 점차 0으로 사라지는 '기울기 소실(Vanishing Gradient)' 문제가 발생한다.  

만약 이 점들을 정규화를 통해 시그모이드의 0에 가까운 위치로 옮긴다면 어떻게 될까?  
시그모이드 함수에서 가장 그래디언트가 큰 부분 주변에 점들이 위치하게 됨으로써, 기울기 소실 문제가 일부 완화될 수 있다.  
- 물론 0.25 정도의 그래디언트만으로는 기울기 소실 문제를 완전히 해결하기 어려워 ReLU와 같은 활성화 함수가 자주 쓰인다.
- 마찬가지로 ReLU의 경우, 배치 정규화는 입력값의 분포가 음수 영역으로 치우치는 것을 막아주므로, 뉴런이 비활성화되는 'Dying ReLU' 현상을 방지하는 데 효과적이다.

하지만, 이러한 강제적인 정규화는 모델의 표현력을 제한하는 문제로 이어진다.  
정규화를 사용한다면 시그모이드 함수의 개형 중 선형 함수와 유사한 아주 일부분만 사용하게 되기 때문이다.  
이는 비선형 함수를 통해 모든 연속 함수를 근사할 수 있다는 Universal Approximation Theorem이 보장하는 신경망의 잠재력을 온전히 발휘하기 어렵게 만든다.  

그래서 기울기 소실 문제를 완화하는 정규화의 이점은 살리면서도, 동시에 모델이 최적의 비선형성을 스스로 찾을 수 있는 유연성을 주기 위해 감마와 베타를 도입한 것이다.

**감마와 베타는 신경망의 다른 가중치처럼 학습 가능한 파라미터(learnable parameter)이며, 역전파 과정에서 손실을 줄이는 방향으로 업데이트된다.**

덕분에 모델은 놀라운 자율성을 얻는다.  
만약 순수한 정규화($\gamma = 1, \beta = 0$)가 최적이라면 그 상태를 유지할 수 있고,  
더 나아가 정규화가 오히려 방해가 된다고 판단되면, 감마와 베타가 원래 입력을 복원하는 항등 함수(Identity Function) 역할을 하도록 학습하여 정규화 계산을 무력화할 수도 있다.

### Training과 Inference에서 다르게 동작한다

위에 서술한 BatchNorm이 작동하는 절차는 오직 학습 과정에 국한된다.  
그 이유는 추론 과정에서 배치 단위로 통계치를 계산하는 행위는 오히려 좋지 못한 결과를 불러올 가능성이 크기 때문이다.  
단순히 생각해보면, 실제 서비스(추론)에서는 동일한 입력에 대해 동일한 출력이 나와야 한다.  
하지만 하나의 사진이 A시간에서는 A배치와 함께 추론에 사용하고, B시간에서는 B배치와 함께 추론에 사용된다면 두 시간에서 동일한 응답이 나올 것이라 기대하기는 어렵다.  
더 나아가 일반적인 추론 환경에서는 입력 데이터가 한 번에 하나씩 들어오는 경우가 많기 때문에 계산에 사용할 배치 자체가 없는 경우가 많다.  

그렇다면, 추론 시에 사용할 통계치는 어떻게 구해야 할까?  

모델은 학습 과정에서 업데이트되는 평균과 분산을 저장하여(**버퍼**) 이 값을 추론 시의 값으로 사용한다.  
이때 평균과 분산을 저장하는 방식은 **지수 이동 평균**을 활용한다.  

물론, 학습이 전부 끝나고 한번에 전체 샘플에 대한 평균과 분산을 계산하거나, 학습 과정에서 기록된 평균과 분산의 모든 히스토리를 메모리에 저장하는 방식도 이론적으로는 가능하다.  
하지만 이 두 방식은 계산 비용의 엄청난 낭비이거나 메모리 용량이 초과될 것이다.  

두 방법과 달리 이동 평균 방식은 점화식으로 나타낼 수 있어 오직 이전 스텝의 값과 현재의 미니 배치 계산 값만 알면 되기에 효율적으로 시간과 메모리를 사용할 수 있다.  
2015년에 발표된 배치 정규화 논문은 단순히 '이동 평균'으로만 언급했지만, PyTorch 라이브러리 상의 구현과 실제 용례는 지수 이동 평균으로 이뤄진다.  

> running_stat = (1 − momentum) * running_stat + momentum * batch_stat

여기서 batch stat은 현재 미니배치에서 계산된 평균 혹은 분산을 의미한다.  
이 업데이트 공식에서 momentum은 새로운 배치의 정보를 얼마나 반영할지에 대한 비율을 의미한다.  
momentum이 0.1이라면 현재의 미니배치 통계치는 10%를 사용하고, 기존의 저장된 정보는 90%를 반영한다.  

이 때, 점화식을 재귀적으로 대입해나가면 k 스텝 전 데이터는 가중치가 momentum * (1-momentum)^k 이고,  
이는 한 스텝이 지나가면 (1-momentum)만큼 해당 데이터의 영향력이 줄어드는 것을 의미한다.  
이를 통해 해당 업데이트 공식이 지수적인 이유를 알 수 있다.  

momentum으로 인해 과거 데이터의 영향력은 점점 줄어들게 되므로, 초기 학습 단계의 불안정한 통계치의 영향력은 시간이 지남에 따라 자연스럽게 감소하고, 학습이 안정된 후반부의 통계치를 더 많이 반영하는 합리적인 값을 얻을 수 있다.  

### Internal Covariate Shift? No, Smoothness of Optimization Landscape

배치 정규화를 제안한 2015년 논문에서는, 배치 정규화가 '내부 공변량 변화(Internal Covariate Shift, ICS)' 현상을 완화하여 학습을 안정시키고 수렴 속도를 높인다고 주장했다.  

그렇다면 내부 공변량 변화(ICS)란 무엇일까?  

딥러닝 모델에서 L+1번째 레이어의 입력은 L번째 레이어의 출력값이다. 이 값은 L번째 레이어의 가중치 W와 편향 b에 대한 연산 결과로 결정된다.  

<img width="381" height="64" alt="image" src="https://github.com/user-attachments/assets/c988f08d-104b-4235-a018-f1eba2bf5b39" />

이때, 역전파를 통해 L번째 레이어의 파라미터가 업데이트되면, 다음 이터레이션에서는 이전과 동일한 입력이 들어와도 L+1번째 레이어가 받는 입력의 통계적 분포는 달라지게 된다.  
이 현상은 L+1번째 레이어가 자신의 파라미터를 최적화하는 동시에, 계속해서 변하는 이전 레이어의 출력 분포에 적응해야 하므로 학습 속도 저하를 야기한다.

배치 정규화를 레이어 사이에 삽입하면 이 입력 분포가 안정되어 학습 속도 저하를 방지할 수 있다는 것이 2015년 논문의 핵심 주장이었다.  

하지만, 2018년 NeurIPS에 게재된 'How Does Batch Normalization Help Optimization?' 논문은 Batch Normalization과 ICS 간의 상관관계가 약하다는 것을 보이며 기존 가설에 의문을 제기했다.  
배치 정규화를 적용했음에도 ICS가 여전하거나 오히려 증가하는 경우를 실험적으로 증명한 것이다.  

해당 논문은 Batch Normalization이 효과적인 진짜 이유는 최적화 지형(Optimization Landscape)을 더 부드럽게(Lipschitzness 향상) 만들어,  
옵티마이저가 더 안정적이고 빠르게 수렴하도록 돕기 때문이라고 주장한다.

>최적화 지형 (Optimization Landscape): 모델의 모든 파라미터 조합에 대해 각각 어떤 손실값을 갖는지 시각화한 지형도

신경망은 여러 레이어가 연결된 복잡한 함수이기에, 앞쪽 레이어의 가중치가 조금만 변해도 뒤쪽 레이어들을 거치며 그 변화가 증폭되어 최종 손실(Loss) 값을 크게 요동치게 만들 수 있다.

하지만, 배치 정규화는 각 레이어의 출력 분포를 안정적인 범위로 조절하는 '브레이크' 역할을 한다.  
앞쪽 레이어의 변화가 크더라도 배치 정규화를 거치면서 그 영향력이 완충되는 것이다.  
즉, 파라미터 변화에 따른 손실 함수의 변화율에 실질적인 상한선을 만들어주어, 최적화 지형을 급격한 변화가 없는 부드러운 상태로 만든다는 것이다.  

### CNN에서의 배치 정규화

입력 데이터가 (32, 3, 3, 3) 행렬이며 컨볼루션 필터의 사이즈는 2x2인 상황을 가정해보자. 이때 필터가 입력 데이터를 훑고 지나가면서 2x2의 출력 결과값을 만들어낸다.  
MLP의 경우에는 각 노드의 출력을 독립적으로 정규화했다면 32개 샘플에 대해 계산한다.

반면, CNN의 경우 2x2 행렬인 출력 결과 또한 정규화 대상에 해당이 되어, **마치 32 * 2 * 2개 샘플을 가진 것처럼 정규화를 진행한다.**
참고한 강의에서 언급한 바로는 "같은 필터 = 같은 감마, 베타"라고 하였으므로 이 점을 숙지하면 될 것이다.

---

### Reference

- https://www.youtube.com/watch?v=m61OSJfxL0U
- https://cvml.tistory.com/6
- https://en.wikipedia.org/wiki/Neural_network_%28machine_learning%29
