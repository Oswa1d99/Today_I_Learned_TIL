### 부동소수점

컴퓨터는 내부적으로 부동 소수점 방식을 통해 소수를 표현한다.  
부동소수점은 소수점이 고정돼 있지 않은 소수 표현 방식으로, 필요에 따라 소수점이 이동하기 때문에 유동적인(floating)한 방식이다.  
> eg. 123.456은 1.23456 x 10^2 으로도 1234.56 x 10^(-1)로도 표현 가능.  
이 때, 2진수를 이용해 가수 x 2^(지수)의 형태를 띄도록 만든 것이 컴퓨터에서의 부동소수점이다.  

### float32 vs float64

쉽게 말해서, 소수를 표현하는 데 얼마나 메모리를 할당한건지에 따라 달려있다.  
float32는 가수 x 2^(지수) 형태를 표현하는 데 32비트(4바이트)를 할당한다.    
<img width="978" height="367" alt="image" src="https://github.com/user-attachments/assets/374e2e06-bec7-4e0d-9078-7c143dce4998" />


float64는 가수 x 2^(지수) 형태를 표현하는 데 64비트(8바이트)를 할당한다.    
<img width="1001" height="383" alt="image" src="https://github.com/user-attachments/assets/f37fac50-6572-4c5b-9727-885f7ab60aac" />


1. 소수의 부호를 판단하는 1비트가 float32와 float64 메모리의 맨 앞단에 붙는다.  
   이 비트가 0이라면 해당 소수가 양수임을 의미하며, 1이라면 해당 소수가 음수임을 의미한다.  

2. 지수는 특이하게도 bias라는 값을 더하여 메모리에 저장한다.  
   이는 음수인 지수와 양수인 지수가 모두 가능하기 때문이다. 예를 들어, 0.11101을 저장한다면 1.1101 x 2^(-1)으로 표현한다.  
   이때 -1을 그대로 지수 메모리에 할당할 수 없기 때문에, bias를 도입하여 이를 표현한다.  
   bias의 계산 값은 다음과 같다:  
   
   2^(지수 표현에 사용되는 비트수 - 1) -1 + 지수  
   > eg. float32에서 0.11101의 bias는 2^(8-1) - 1 + (-1)  

4. 가수는 1.xxx 꼴의 '정규화된 수'의 형태로 23비트에 나타낸다.  
   형태를 고정했으니 정수 부분에 해당하는 1.은 저장할 필요가 없고, 소수 부분인 xxx에 해당하는 값만 저장한다.  
   만약, 23비트가 다 채워지지 않는다면 뒤에 0으로 패딩하여 23비트를 채운다.  
  
### In Python

파이썬을 처음 접할 때 이해하기 어려웠던 문제가 있다.  
```
print(0.1 + 0.2 == 0.3)
```
직관적으로 보았을 때는 True가 출력되겠지만, **float64**를 채택하는 많은 프로그래밍 언어에서는 False가 출력된다.  
  
이는 0.1 + 0.2가 0.30000000000000004의 값을 가지기 때문인데, 0.1과 0.2를 10진수의 가수와 진수 형태로 표현할 수는 있지만 2진수를 채택했을 때는 둘 모두 **무한 소수**이기 때문이다.  
  
이 무한 소수를 64비트에 맞게 근사치로 잘라낸 후, 이 값들을 더한 결과를 10진수로 표현하기 때문에 반직관적인 결과가 출력된 것이다.  

### In PyTorch

PyTorch에서는 파이썬과 다르게 **float32**이 기본값이다.  
  
이는 PyTorch에서 텐서 연산에 주로 사용되는 GPU의 특성에 의존하기 때문이다.  
지연 시간을 줄이고 메모리 접근 횟수를 최소화하려는 CPU의 목적과 다르게, 
GPU는 **처리량(Throughput)을 최대화**해 병렬 연산의 장점을 살리는 것을 목적으로 한다.  
  
이 때, float64는 당연하게도 float32보다 두 배의 메모리를 차지하는데,  
이는 GPU 메모리에 최대한 많은 연산을 적재하고자 하는 GPU의 목적과는 부합하지 않는다. 
그렇기에 GPU 내부의 텐서 코어(NVIDIA GPU의 코어 중 행렬 연산에 특화된 장치)는 float16과 float32을 혼용하는 **혼합 정밀도 계산** (Mixed-Precision Computing)을 따른다.  

또한, float64를 담당해 처리하는 연산 유닛은 매우 적거나 거의 없기 때문에, 일반적으로 PyTorch에서 float의 dtype은 float32로 설정한다.  






